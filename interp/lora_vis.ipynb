{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from huggingface_hub import snapshot_download\n",
    "from safetensors.torch import load_file\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import re\n",
    "from collections import defaultdict\n",
    "import json\n",
    "import time\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model_and_tokenizer(base_model_id, load_in_4bit=False):\n",
    "    \"\"\"Load a model and tokenizer from HuggingFace.\"\"\"\n",
    "    print(f\"Loading tokenizer from {base_model_id}...\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(base_model_id)\n",
    "    \n",
    "    # Set quantization config if needed\n",
    "    if load_in_4bit:\n",
    "        from transformers import BitsAndBytesConfig\n",
    "        quantization_config = BitsAndBytesConfig(\n",
    "            load_in_4bit=True,\n",
    "            bnb_4bit_compute_dtype=torch.float16,\n",
    "            bnb_4bit_quant_type=\"nf4\",\n",
    "            bnb_4bit_use_double_quant=False\n",
    "        )\n",
    "    else:\n",
    "        quantization_config = None\n",
    "    \n",
    "    # Load model\n",
    "    print(f\"Loading model from {base_model_id}...\")\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        base_model_id,\n",
    "        device_map=\"auto\",\n",
    "        torch_dtype=torch.float16,\n",
    "        quantization_config=quantization_config,\n",
    "        trust_remote_code=True\n",
    "    )\n",
    "    \n",
    "    return model, tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def explore_model_structure(model, max_depth=3, current_depth=0, path=\"model\"):\n",
    "    \"\"\"Recursively explore and print the model structure.\"\"\"\n",
    "    if current_depth >= max_depth:\n",
    "        return\n",
    "    \n",
    "    try:\n",
    "        # Print attributes at this level\n",
    "        attrs = dir(model)\n",
    "        for attr in attrs:\n",
    "            if attr.startswith('_'):\n",
    "                continue\n",
    "            try:\n",
    "                value = getattr(model, attr)\n",
    "                type_name = type(value).__name__\n",
    "                if hasattr(value, 'shape'):\n",
    "                    print(f\"{' ' * current_depth * 2}{path}.{attr}: {type_name} (shape: {value.shape})\")\n",
    "                else:\n",
    "                    print(f\"{' ' * current_depth * 2}{path}.{attr}: {type_name}\")\n",
    "                \n",
    "                # Recursively explore non-primitive types\n",
    "                if not attr.startswith('_') and not callable(value) and not isinstance(value, (str, int, float, bool)):\n",
    "                    explore_model_structure(value, max_depth, current_depth + 1, f\"{path}.{attr}\")\n",
    "            except Exception as e:\n",
    "                print(f\"{' ' * current_depth * 2}{path}.{attr}: [Error accessing: {e}]\")\n",
    "    except Exception as e:\n",
    "        print(f\"{' ' * current_depth * 2}{path}: [Error exploring: {e}]\")\n",
    "\n",
    "def analyze_model_structure(model):\n",
    "    \"\"\"Analyze the top-level structure of the model.\"\"\"\n",
    "    print(\"\\nModel Structure Analysis:\")\n",
    "    print(f\"Model type: {type(model).__name__}\")\n",
    "    print(f\"Has 'model' attribute: {hasattr(model, 'model')}\")\n",
    "    \n",
    "    if hasattr(model, 'model'):\n",
    "        print(f\"model.model type: {type(model.model).__name__}\")\n",
    "        print(f\"Has 'model.model' attribute: {hasattr(model.model, 'model')}\")\n",
    "        \n",
    "        if hasattr(model.model, 'model'):\n",
    "            print(f\"model.model.model type: {type(model.model.model).__name__}\")\n",
    "            print(f\"Has 'layers' attribute: {hasattr(model.model.model, 'layers')}\")\n",
    "            \n",
    "            if hasattr(model.model.model, 'layers'):\n",
    "                layers = model.model.model.layers\n",
    "                if isinstance(layers, dict):\n",
    "                    print(f\"First few layer keys: {list(layers.keys())[:5]}\")\n",
    "                else:\n",
    "                    print(f\"Layers type: {type(layers).__name__}\")\n",
    "                    if hasattr(layers, '__len__'):\n",
    "                        print(f\"Number of layers: {len(layers)}\")\n",
    "                        if len(layers) > 0:\n",
    "                            print(f\"First layer type: {type(layers[0]).__name__}\")\n",
    "                            print(f\"First layer attributes: {[attr for attr in dir(layers[0]) if not attr.startswith('_')][:10]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_adapter(adapter_id, local_dir=None):\n",
    "    \"\"\"Download LoRA adapter files from HuggingFace.\"\"\"\n",
    "    print(f\"Downloading adapter files from {adapter_id}...\")\n",
    "    \n",
    "    if local_dir is None:\n",
    "        local_dir = os.path.join(\"adapters\", os.path.basename(adapter_id))\n",
    "    \n",
    "    # Download the repository\n",
    "    path = snapshot_download(\n",
    "        repo_id=adapter_id,\n",
    "        token=os.environ.get(\"HF_TOKEN\"),\n",
    "        local_dir=local_dir,\n",
    "        ignore_patterns=[\"*.bin\", \"*.model\", \"optimizer.pt\", \"*.md\", \"*.txt\"]\n",
    "    )\n",
    "    \n",
    "    print(f\"Downloaded adapter files to {path}\")\n",
    "    return path\n",
    "\n",
    "def load_adapter_weights(adapter_path):\n",
    "    \"\"\"Load LoRA adapter weights from either .safetensors or .bin files.\"\"\"\n",
    "    adapter_weights = {}\n",
    "    \n",
    "    # Look for safetensors or pytorch bin files\n",
    "    files = list(Path(adapter_path).glob(\"*.safetensors\"))\n",
    "    \n",
    "    if not files:\n",
    "        files = list(Path(adapter_path).glob(\"adapter_model.bin\"))\n",
    "        if files:\n",
    "            adapter_weights = torch.load(files[0], map_location=\"cpu\")\n",
    "    else:\n",
    "        for file in files:\n",
    "            tensors = load_file(file)\n",
    "            adapter_weights.update(tensors)\n",
    "    \n",
    "    if not adapter_weights:\n",
    "        raise FileNotFoundError(f\"No adapter weights found in {adapter_path}\")\n",
    "        \n",
    "    return adapter_weights\n",
    "\n",
    "def analyze_adapter_keys(adapter_weights):\n",
    "    \"\"\"Analyze the structure of keys in the adapter weights.\"\"\"\n",
    "    # Get all keys\n",
    "    keys = list(adapter_weights.keys())\n",
    "    print(f\"Total keys in adapter: {len(keys)}\")\n",
    "    \n",
    "    # Print a few sample keys\n",
    "    print(\"Sample keys:\")\n",
    "    for key in keys[:5]:\n",
    "        print(f\"  {key}\")\n",
    "    \n",
    "    # Extract base layer names (before lora_A/lora_B)\n",
    "    base_layers = set()\n",
    "    for key in keys:\n",
    "        if 'lora_A' in key:\n",
    "            base_name = key.split('.lora_A')[0]\n",
    "            base_layers.add(base_name)\n",
    "    \n",
    "    print(f\"\\nUnique base layer names: {len(base_layers)}\")\n",
    "    print(\"Sample base layer names:\")\n",
    "    for base in list(base_layers)[:5]:\n",
    "        print(f\"  {base}\")\n",
    "    \n",
    "    # Extract common prefixes\n",
    "    prefixes = set()\n",
    "    for base in base_layers:\n",
    "        parts = base.split('.')\n",
    "        if len(parts) > 0:\n",
    "            prefixes.add(parts[0])\n",
    "    \n",
    "    print(f\"\\nCommon prefixes: {prefixes}\")\n",
    "    \n",
    "    # Analyze layer numbers\n",
    "    layer_numbers = set()\n",
    "    pattern = r'layers\\.(\\d+)'\n",
    "    for base in base_layers:\n",
    "        match = re.search(pattern, base)\n",
    "        if match:\n",
    "            layer_numbers.add(int(match.group(1)))\n",
    "    \n",
    "    print(f\"\\nLayer numbers found: {sorted(layer_numbers)}\")\n",
    "    \n",
    "    return {\n",
    "        'keys': keys,\n",
    "        'base_layers': base_layers,\n",
    "        'prefixes': prefixes,\n",
    "        'layer_numbers': sorted(layer_numbers)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_base_weights_for_layer(model, layer_name, debug=False):\n",
    "    \"\"\"Extract the base weights for a specific layer by name with better debugging.\"\"\"\n",
    "    if debug:\n",
    "        print(f\"Attempting to access base weights for: {layer_name}\")\n",
    "    \n",
    "    # Try different transformations of the layer name\n",
    "    name_mappings = [\n",
    "        # Original mapping attempt\n",
    "        lambda name: name,\n",
    "        # Remove base_model prefix\n",
    "        lambda name: name.replace('base_model.', '', 1),\n",
    "        # Remove model.model prefix\n",
    "        lambda name: name.replace('model.model.', '', 1),\n",
    "        # Just keep the last parts (layer number + module type)\n",
    "        lambda name: '.'.join(name.split('.')[-3:])\n",
    "    ]\n",
    "    \n",
    "    for mapping_func in name_mappings:\n",
    "        transformed_name = mapping_func(layer_name)\n",
    "        if debug:\n",
    "            print(f\"  Trying transformed name: {transformed_name}\")\n",
    "        \n",
    "        # Try to navigate through the model hierarchy\n",
    "        try:\n",
    "            parts = transformed_name.split('.')\n",
    "            current_module = model\n",
    "            \n",
    "            for part in parts:\n",
    "                if part.isdigit():\n",
    "                    current_module = current_module[int(part)]\n",
    "                else:\n",
    "                    current_module = getattr(current_module, part)\n",
    "            \n",
    "            # Return the weight tensor if we found it\n",
    "            if hasattr(current_module, 'weight'):\n",
    "                if debug:\n",
    "                    print(f\"  SUCCESS: Found weights using {transformed_name}\")\n",
    "                return current_module.weight.detach().cpu()\n",
    "            else:\n",
    "                if debug:\n",
    "                    print(f\"  Found module but it has no weight attribute\")\n",
    "        except (AttributeError, IndexError, KeyError) as e:\n",
    "            if debug:\n",
    "                print(f\"  Failed with {e}\")\n",
    "    \n",
    "    # If we get here, we couldn't find the weights\n",
    "    if debug:\n",
    "        print(f\"  WARNING: Could not find base weights for {layer_name}\")\n",
    "    return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_weight_access(model, adapter_base_layers, debug=True):\n",
    "    \"\"\"Test accessing base weights for a sample of adapter layers.\"\"\"\n",
    "    successful = 0\n",
    "    failed = 0\n",
    "    \n",
    "    # Test on a sample of layers\n",
    "    sample_size = min(10, len(adapter_base_layers))\n",
    "    sample_layers = list(adapter_base_layers)[:sample_size]\n",
    "    \n",
    "    for layer_name in sample_layers:\n",
    "        weights = get_base_weights_for_layer(model, layer_name, debug=debug)\n",
    "        if weights is not None:\n",
    "            successful += 1\n",
    "        else:\n",
    "            failed += 1\n",
    "    \n",
    "    print(f\"\\nWeight access test results:\")\n",
    "    print(f\"  Successful: {successful}/{sample_size}\")\n",
    "    print(f\"  Failed: {failed}/{sample_size}\")\n",
    "    \n",
    "    return successful, failed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_relative_impact(base_model_id, adapter_id, output_dir=\"relative_impact_analysis\", \n",
    "                           sample_rate=0.25, load_base_model=True, debug=False):\n",
    "    \"\"\"Analyze the relative impact of LoRA adapters compared to base model weights.\"\"\"\n",
    "    print(\"\\n[1/7] Starting relative impact analysis...\")\n",
    "    \n",
    "    # Create output directory\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    print(f\"Output directory created: {output_dir}\")\n",
    "    \n",
    "    # Load the base model if requested\n",
    "    model = None\n",
    "    if load_base_model:\n",
    "        print(f\"[2/7] Loading base model from {base_model_id}...\")\n",
    "        try:\n",
    "            model, _ = load_model_and_tokenizer(base_model_id, load_in_4bit=True)\n",
    "            print(\"Successfully loaded base model\")\n",
    "            \n",
    "            # Analyze model structure\n",
    "            if debug:\n",
    "                analyze_model_structure(model)\n",
    "                print(\"\\nExploring model structure (first 2 levels):\")\n",
    "                explore_model_structure(model, max_depth=2)\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading base model: {e}\")\n",
    "            print(\"Continuing without base model comparison...\")\n",
    "            load_base_model = False\n",
    "    \n",
    "    # Get the adapter path\n",
    "    print(f\"[3/7] Loading LoRA adapter from {adapter_id}...\")\n",
    "    adapter_path = download_adapter(adapter_id)\n",
    "    \n",
    "    # Load the adapter weights\n",
    "    adapter_weights = load_adapter_weights(adapter_path)\n",
    "    print(f\"Successfully loaded adapter weights with {len(adapter_weights)} tensors\")\n",
    "    \n",
    "    # Analyze adapter keys\n",
    "    if debug:\n",
    "        adapter_info = analyze_adapter_keys(adapter_weights)\n",
    "        \n",
    "        # Test weight access if base model is loaded\n",
    "        if load_base_model and model is not None:\n",
    "            successful, failed = test_weight_access(model, adapter_info['base_layers'], debug=True)\n",
    "    \n",
    "    # Extract adapter configuration if available\n",
    "    adapter_config = {}\n",
    "    config_files = list(Path(adapter_path).glob(\"adapter_config.json\"))\n",
    "    if config_files:\n",
    "        with open(config_files[0], 'r') as f:\n",
    "            adapter_config = json.load(f)\n",
    "        \n",
    "        print(\"\\nAdapter Configuration:\")\n",
    "        for key, value in adapter_config.items():\n",
    "            print(f\"  {key}: {value}\")\n",
    "    \n",
    "    # Extract layer info\n",
    "    layer_info = []\n",
    "    \n",
    "    # Track target modules from config\n",
    "    target_modules = adapter_config.get(\"target_modules\", [])\n",
    "    \n",
    "    # Regular expression to find lora_A tensors\n",
    "    lora_a_pattern = re.compile(r'.*lora_A.*')\n",
    "    \n",
    "    # Find all lora_A keys\n",
    "    lora_a_keys = [k for k in adapter_weights.keys() if lora_a_pattern.match(k)]\n",
    "    total_layers = len(lora_a_keys)\n",
    "    \n",
    "    print(f\"\\n[4/7] Found {total_layers} LoRA layer pairs\")\n",
    "    print(f\"Analyzing approximately {int(total_layers * sample_rate)} layers (sample rate: {sample_rate*100:.0f}%)\")\n",
    "    \n",
    "    # Sample layers to analyze\n",
    "    if sample_rate < 1.0:\n",
    "        print(\"Ensuring representative sampling across layer depths...\")\n",
    "        # Ensure we get a representative sample across all layer numbers\n",
    "        layer_nums = {}\n",
    "        for key in lora_a_keys:\n",
    "            match = re.search(r'layers\\.(\\d+)', key)\n",
    "            if match:\n",
    "                layer_num = int(match.group(1))\n",
    "                if layer_num not in layer_nums:\n",
    "                    layer_nums[layer_num] = []\n",
    "                layer_nums[layer_num].append(key)\n",
    "        \n",
    "        # Sample from each layer number\n",
    "        sampled_keys = []\n",
    "        for num, keys in layer_nums.items():\n",
    "            num_to_sample = max(1, int(len(keys) * sample_rate))\n",
    "            sampled_keys.extend(np.random.choice(keys, size=num_to_sample, replace=False))\n",
    "        \n",
    "        lora_a_keys = sampled_keys\n",
    "        print(f\"Sampled {len(lora_a_keys)} layers across {len(layer_nums)} different layer depths\")\n",
    "    \n",
    "    # Process the sampled layers\n",
    "    print(\"[5/7] Analyzing LoRA layers...\")\n",
    "    \n",
    "    # Track debugging info when using debug mode\n",
    "    weight_access_results = {\"success\": 0, \"failed\": 0}\n",
    "    \n",
    "    # Use tqdm for a progress bar\n",
    "    for a_key in tqdm(lora_a_keys, desc=\"Analyzing layers\"):\n",
    "        # Find the corresponding B matrix key\n",
    "        b_key = a_key.replace('lora_A', 'lora_B')\n",
    "        \n",
    "        if b_key in adapter_weights:\n",
    "            # Extract base layer name and module type\n",
    "            base_name = a_key.split('.lora_A')[0]\n",
    "            \n",
    "            # Extract layer type\n",
    "            layer_type = \"unknown\"\n",
    "            for module in target_modules:\n",
    "                if f\".{module}\" in base_name:\n",
    "                    layer_type = module\n",
    "                    break\n",
    "            \n",
    "            # Extract layer number\n",
    "            match = re.search(r'layers\\.(\\d+)', base_name)\n",
    "            layer_num = int(match.group(1)) if match else -1\n",
    "            \n",
    "            # Get shapes without computing norms\n",
    "            a_tensor = adapter_weights[a_key]\n",
    "            b_tensor = adapter_weights[b_key]\n",
    "            \n",
    "            # Calculate metrics without full matrix multiplication\n",
    "            a_norm = torch.norm(a_tensor).item()\n",
    "            b_norm = torch.norm(b_tensor).item()\n",
    "            \n",
    "            # Estimate of the Frobenius norm (upper bound) without full multiplication\n",
    "            est_frob_norm = a_norm * b_norm\n",
    "            \n",
    "            # Get base model weights for comparison if requested\n",
    "            base_weight_norm = None\n",
    "            relative_impact = None\n",
    "            \n",
    "            if load_base_model and model is not None:\n",
    "                # Try to find the corresponding base model weight\n",
    "                base_weights = get_base_weights_for_layer(model, base_name, debug=debug)\n",
    "                \n",
    "                if base_weights is not None:\n",
    "                    weight_access_results[\"success\"] += 1\n",
    "                    base_weight_norm = torch.norm(base_weights).item()\n",
    "                    relative_impact = (est_frob_norm / base_weight_norm) * 100  # as percentage\n",
    "                else:\n",
    "                    weight_access_results[\"failed\"] += 1\n",
    "            \n",
    "            layer_info.append({\n",
    "                'layer_name': base_name,\n",
    "                'layer_type': layer_type,\n",
    "                'layer_num': layer_num,\n",
    "                'a_shape': list(a_tensor.shape),\n",
    "                'b_shape': list(b_tensor.shape),\n",
    "                'rank': a_tensor.shape[0],\n",
    "                'param_count': a_tensor.numel() + b_tensor.numel(),\n",
    "                'a_norm': a_norm,\n",
    "                'b_norm': b_norm,\n",
    "                'est_frob_norm': est_frob_norm,\n",
    "                'base_weight_norm': base_weight_norm,\n",
    "                'relative_impact_pct': relative_impact\n",
    "            })\n",
    "    \n",
    "    if debug and load_base_model:\n",
    "        print(f\"\\nBase weight access results:\")\n",
    "        print(f\"  Successful accesses: {weight_access_results['success']}\")\n",
    "        print(f\"  Failed accesses: {weight_access_results['failed']}\")\n",
    "        if weight_access_results['success'] == 0:\n",
    "            print(\"  WARNING: Could not access any base weights. Relative impact analysis will be unavailable.\")\n",
    "    \n",
    "    print(f\"  Completed analysis of {len(layer_info)} layers\")\n",
    "    \n",
    "    # Convert to DataFrame\n",
    "    metrics_df = pd.DataFrame(layer_info)\n",
    "    \n",
    "    # Add scaling factor for sampling\n",
    "    scaling_factor = 1.0 / sample_rate if sample_rate < 1.0 else 1.0\n",
    "    \n",
    "    # Save full metrics\n",
    "    metrics_df.to_csv(os.path.join(output_dir, \"layer_metrics_sampled.csv\"), index=False)\n",
    "    \n",
    "    # Summary statistics by layer type\n",
    "    layer_type_summary = {}\n",
    "    for layer_type, group in metrics_df.groupby('layer_type'):\n",
    "        # For relative impact, only include rows where we have the data\n",
    "        rel_impact_data = group[group['relative_impact_pct'].notna()]\n",
    "        \n",
    "        layer_type_summary[layer_type] = {\n",
    "            'mean_norm': group['est_frob_norm'].mean(),\n",
    "            'sum_norm': group['est_frob_norm'].sum() * scaling_factor,\n",
    "            'count': len(group) * scaling_factor,\n",
    "            'param_count': group['param_count'].sum() * scaling_factor\n",
    "        }\n",
    "        \n",
    "        # Add relative impact stats if we have them\n",
    "        if load_base_model and not rel_impact_data.empty:\n",
    "            layer_type_summary[layer_type].update({\n",
    "                'mean_relative_pct': rel_impact_data['relative_impact_pct'].mean(),\n",
    "                'max_relative_pct': rel_impact_data['relative_impact_pct'].max(),\n",
    "                'median_relative_pct': rel_impact_data['relative_impact_pct'].median()\n",
    "            })\n",
    "    \n",
    "    # Convert to DataFrame\n",
    "    layer_type_stats = pd.DataFrame.from_dict(layer_type_summary, orient='index')\n",
    "    \n",
    "    # Get stats by layer number\n",
    "    layer_num_summary = {}\n",
    "    for layer_num, group in metrics_df.groupby('layer_num'):\n",
    "        # For relative impact, only include rows where we have the data\n",
    "        rel_impact_data = group[group['relative_impact_pct'].notna()]\n",
    "        \n",
    "        layer_num_summary[layer_num] = {\n",
    "            'mean_norm': group['est_frob_norm'].mean(),\n",
    "            'sum_norm': group['est_frob_norm'].sum() * scaling_factor,\n",
    "            'count': len(group) * scaling_factor,\n",
    "            'param_count': group['param_count'].sum() * scaling_factor\n",
    "        }\n",
    "        \n",
    "        # Add relative impact stats if we have them\n",
    "        if load_base_model and not rel_impact_data.empty:\n",
    "            layer_num_summary[layer_num].update({\n",
    "                'mean_relative_pct': rel_impact_data['relative_impact_pct'].mean(),\n",
    "                'max_relative_pct': rel_impact_data['relative_impact_pct'].max()\n",
    "            })\n",
    "    \n",
    "    # Convert to DataFrame\n",
    "    layer_num_stats = pd.DataFrame.from_dict(layer_num_summary, orient='index').reset_index()\n",
    "    layer_num_stats.columns = ['layer_num'] + list(layer_num_stats.columns)[1:]\n",
    "    layer_num_stats = layer_num_stats.sort_values('layer_num')\n",
    "    \n",
    "    # Get top layers\n",
    "    top_layers_rel = None\n",
    "    if load_base_model:\n",
    "        # Sort by relative impact if available\n",
    "        rel_impact_df = metrics_df[metrics_df['relative_impact_pct'].notna()]\n",
    "        if not rel_impact_df.empty:\n",
    "            top_layers_rel = rel_impact_df.sort_values('relative_impact_pct', ascending=False).head(10)\n",
    "    \n",
    "    # Also get top by absolute impact\n",
    "    top_layers_abs = metrics_df.sort_values('est_frob_norm', ascending=False).head(10)\n",
    "    \n",
    "    print(\"[6/7] Generating visualizations...\")\n",
    "    \n",
    "    # 1. Impact by layer type bar chart\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    types_df = layer_type_stats.sort_values('sum_norm', ascending=False)\n",
    "    \n",
    "    # Create bar chart for absolute impact\n",
    "    plt.subplot(1, 2, 1)\n",
    "    sns.barplot(\n",
    "        x=types_df.index,\n",
    "        y=types_df['sum_norm']\n",
    "    )\n",
    "    plt.title('Absolute Impact by Layer Type')\n",
    "    plt.xlabel('Layer Type')\n",
    "    plt.ylabel('Sum of Estimated Norms')\n",
    "    plt.xticks(rotation=45)\n",
    "    \n",
    "    # Create bar chart for relative impact if available\n",
    "    if load_base_model and 'mean_relative_pct' in types_df.columns:\n",
    "        plt.subplot(1, 2, 2)\n",
    "        sorted_by_rel = types_df.sort_values('mean_relative_pct', ascending=False)\n",
    "        sns.barplot(\n",
    "            x=sorted_by_rel.index,\n",
    "            y=sorted_by_rel['mean_relative_pct']\n",
    "        )\n",
    "        plt.title('Relative Impact by Layer Type (% of Base Weight)')\n",
    "        plt.xlabel('Layer Type')\n",
    "        plt.ylabel('Mean Relative Impact (%)')\n",
    "        plt.xticks(rotation=45)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(output_dir, \"impact_by_layer_type.png\"))\n",
    "    \n",
    "    # Generate summary text report\n",
    "    print(\"[7/7] Generating summary report...\")\n",
    "    with open(os.path.join(output_dir, \"analysis_summary.txt\"), 'w') as f:\n",
    "        f.write(\"LoRA Adapter Relative Impact Analysis\\n\")\n",
    "        f.write(\"===================================\\n\\n\")\n",
    "        \n",
    "        f.write(f\"Base Model: {base_model_id}\\n\")\n",
    "        f.write(f\"Adapter: {adapter_id}\\n\\n\")\n",
    "        \n",
    "        f.write(f\"Total LoRA layers: {total_layers}\\n\")\n",
    "        f.write(f\"Analyzed: {len(metrics_df)} layers ({sample_rate*100:.0f}% sample)\\n\")\n",
    "        f.write(f\"Estimated total parameters: {int(metrics_df['param_count'].sum() * scaling_factor):,}\\n\\n\")\n",
    "        \n",
    "        # Sort by impact\n",
    "        types_df = layer_type_stats.sort_values('sum_norm', ascending=False)\n",
    "        \n",
    "        f.write(\"Impact by Layer Type (ordered by estimated total norm):\\n\")\n",
    "        for layer_type, row in types_df.iterrows():\n",
    "            total_norm = row['sum_norm']\n",
    "            count = row['count']\n",
    "            params = row['param_count']\n",
    "            f.write(f\"  {layer_type}: {total_norm:.2f} est. norm, ~{count:.1f} layers, ~{params:,.0f} parameters\\n\")\n",
    "            \n",
    "            # Add relative impact if available\n",
    "            if load_base_model and 'mean_relative_pct' in row:\n",
    "                mean_rel = row['mean_relative_pct']\n",
    "                max_rel = row['max_relative_pct']\n",
    "                f.write(f\"    Relative Impact: {mean_rel:.2f}% avg, {max_rel:.2f}% max of base weights\\n\")\n",
    "        \n",
    "        # Print top layers by absolute impact\n",
    "        f.write(\"\\nTop 10 Individual Layers by Absolute Impact:\\n\")\n",
    "        for _, row in top_layers_abs.iterrows():\n",
    "            name = row['layer_name']\n",
    "            impact = row['est_frob_norm']\n",
    "            f.write(f\"  {name}: {impact:.2f} est. norm\\n\")\n",
    "            \n",
    "            # Add relative impact if available\n",
    "            if load_base_model and 'relative_impact_pct' in row and not pd.isna(row['relative_impact_pct']):\n",
    "                rel_impact = row['relative_impact_pct']\n",
    "                f.write(f\"    {rel_impact:.2f}% of base weight\\n\")\n",
    "        \n",
    "        # Print top layers by relative impact if available\n",
    "        if load_base_model and top_layers_rel is not None and not top_layers_rel.empty:\n",
    "            f.write(\"\\nTop 10 Individual Layers by Relative Impact (% of base weight):\\n\")\n",
    "            for _, row in top_layers_rel.iterrows():\n",
    "                name = row['layer_name']\n",
    "                rel_impact = row['relative_impact_pct']\n",
    "                abs_impact = row['est_frob_norm']\n",
    "                f.write(f\"  {name}: {rel_impact:.2f}% of base weight (abs: {abs_impact:.2f})\\n\")\n",
    "    \n",
    "    # Clean up resources\n",
    "    if load_base_model and model is not None:\n",
    "        # Free up GPU memory\n",
    "        try:\n",
    "            del model\n",
    "            import gc\n",
    "            gc.collect()\n",
    "            torch.cuda.empty_cache()\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "    print(f\"\\n✅ Analysis complete! Results saved to {output_dir}/\")\n",
    "    print(f\"   - CSV data: {os.path.join(output_dir, 'layer_metrics_sampled.csv')}\")\n",
    "    print(f\"   - Summary: {os.path.join(output_dir, 'analysis_summary.txt')}\")\n",
    "    print(f\"   - Visualizations: \")\n",
    "    print(f\"     - {os.path.join(output_dir, 'impact_by_layer_type.png')}\")\n",
    "    \n",
    "    # Return summary DataFrames\n",
    "    return {\n",
    "        'layer_metrics': metrics_df,\n",
    "        'layer_type_stats': layer_type_stats,\n",
    "        'layer_num_stats': layer_num_stats,\n",
    "        'top_layers_abs': top_layers_abs,\n",
    "        'top_layers_rel': top_layers_rel if load_base_model and top_layers_rel is not None and not top_layers_rel.empty else None\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[1/7] Starting relative impact analysis...\n",
      "Output directory created: relative_impact_analysis\n",
      "[2/7] Loading base model from unsloth/Qwen2.5-Coder-32B-Instruct...\n",
      "Loading tokenizer from unsloth/Qwen2.5-Coder-32B-Instruct...\n",
      "Loading model from unsloth/Qwen2.5-Coder-32B-Instruct...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "79963189b0d0478dba01e626fc527203",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/14 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded base model\n",
      "\n",
      "Model Structure Analysis:\n",
      "Model type: Qwen2ForCausalLM\n",
      "Has 'model' attribute: True\n",
      "model.model type: Qwen2Model\n",
      "Has 'model.model' attribute: False\n",
      "\n",
      "Exploring model structure (first 2 levels):\n",
      "model.T_destination: TypeVar\n",
      "model.active_adapter: method\n",
      "model.active_adapters: method\n",
      "model.add_adapter: method\n",
      "model.add_memory_hooks: method\n",
      "model.add_model_tags: method\n",
      "model.add_module: method\n",
      "model.apply: method\n",
      "model.base_model: Qwen2Model\n",
      "model.base_model_prefix: str\n",
      "model.bfloat16: method\n",
      "model.buffers: method\n",
      "model.call_super_init: bool\n",
      "model.can_generate: method\n",
      "model.children: method\n",
      "model.compile: method\n",
      "model.compute_transition_scores: method\n",
      "model.config: Qwen2Config\n",
      "  model.config.add_cross_attention: bool\n",
      "  model.config.architectures: list\n",
      "  model.config.attention_dropout: float\n",
      "  model.config.attribute_map: dict\n",
      "  model.config.bad_words_ids: NoneType\n",
      "  model.config.base_config_key: str\n",
      "  model.config.base_model_pp_plan: dict\n",
      "  model.config.base_model_tp_plan: dict\n",
      "  model.config.begin_suppress_tokens: NoneType\n",
      "  model.config.bos_token_id: int\n",
      "  model.config.chunk_size_feed_forward: int\n",
      "  model.config.cross_attention_hidden_size: NoneType\n",
      "  model.config.decoder_start_token_id: NoneType\n",
      "  model.config.dict_torch_dtype_to_str: method\n",
      "  model.config.diversity_penalty: float\n",
      "  model.config.do_sample: bool\n",
      "  model.config.early_stopping: bool\n",
      "  model.config.encoder_no_repeat_ngram_size: int\n",
      "  model.config.eos_token_id: int\n",
      "  model.config.exponential_decay_length_penalty: NoneType\n",
      "  model.config.finetuning_task: NoneType\n",
      "  model.config.forced_bos_token_id: NoneType\n",
      "  model.config.forced_eos_token_id: NoneType\n",
      "  model.config.from_dict: method\n",
      "  model.config.from_json_file: method\n",
      "  model.config.from_pretrained: method\n",
      "  model.config.get_config_dict: method\n",
      "  model.config.get_text_config: method\n",
      "  model.config.hidden_act: str\n",
      "  model.config.hidden_size: int\n",
      "  model.config.id2label: dict\n",
      "  model.config.initializer_range: float\n",
      "  model.config.intermediate_size: int\n",
      "  model.config.is_composition: bool\n",
      "  model.config.is_decoder: bool\n",
      "  model.config.is_encoder_decoder: bool\n",
      "  model.config.keys_to_ignore_at_inference: list\n",
      "  model.config.label2id: dict\n",
      "  model.config.length_penalty: float\n",
      "  model.config.max_length: int\n",
      "  model.config.max_position_embeddings: int\n",
      "  model.config.max_window_layers: int\n",
      "  model.config.min_length: int\n",
      "  model.config.model_type: str\n",
      "  model.config.name_or_path: str\n",
      "  model.config.no_repeat_ngram_size: int\n",
      "  model.config.num_attention_heads: int\n",
      "  model.config.num_beam_groups: int\n",
      "  model.config.num_beams: int\n",
      "  model.config.num_hidden_layers: int\n",
      "  model.config.num_key_value_heads: int\n",
      "  model.config.num_labels: int\n",
      "  model.config.num_return_sequences: int\n",
      "  model.config.output_attentions: bool\n",
      "  model.config.output_hidden_states: bool\n",
      "  model.config.output_scores: bool\n",
      "  model.config.pad_token_id: int\n",
      "  model.config.prefix: NoneType\n",
      "  model.config.problem_type: NoneType\n",
      "  model.config.pruned_heads: dict\n",
      "  model.config.push_to_hub: method\n",
      "  model.config.quantization_config: BitsAndBytesConfig\n",
      "  model.config.register_for_auto_class: method\n",
      "  model.config.remove_invalid_values: bool\n",
      "  model.config.repetition_penalty: float\n",
      "  model.config.return_dict: bool\n",
      "  model.config.return_dict_in_generate: bool\n",
      "  model.config.rms_norm_eps: float\n",
      "  model.config.rope_scaling: NoneType\n",
      "  model.config.rope_theta: float\n",
      "  model.config.save_pretrained: method\n",
      "  model.config.sep_token_id: NoneType\n",
      "  model.config.sliding_window: NoneType\n",
      "  model.config.sub_configs: dict\n",
      "  model.config.suppress_tokens: NoneType\n",
      "  model.config.task_specific_params: NoneType\n",
      "  model.config.temperature: float\n",
      "  model.config.tf_legacy_loss: bool\n",
      "  model.config.tie_encoder_decoder: bool\n",
      "  model.config.tie_word_embeddings: bool\n",
      "  model.config.to_dict: method\n",
      "  model.config.to_diff_dict: method\n",
      "  model.config.to_json_file: method\n",
      "  model.config.to_json_string: method\n",
      "  model.config.tokenizer_class: NoneType\n",
      "  model.config.top_k: int\n",
      "  model.config.top_p: float\n",
      "  model.config.torch_dtype: dtype\n",
      "  model.config.torchscript: bool\n",
      "  model.config.transformers_version: str\n",
      "  model.config.typical_p: float\n",
      "  model.config.unsloth_fixed: bool\n",
      "  model.config.update: method\n",
      "  model.config.update_from_string: method\n",
      "  model.config.use_bfloat16: bool\n",
      "  model.config.use_cache: bool\n",
      "  model.config.use_return_dict: bool\n",
      "  model.config.use_sliding_window: bool\n",
      "  model.config.vocab_size: int\n",
      "model.config_class: type\n",
      "model.cpu: method\n",
      "model.create_extended_attention_mask_for_decoder: function\n",
      "model.cuda: method\n",
      "model.delete_adapter: method\n",
      "model.dequantize: method\n",
      "model.device: device\n",
      "  model.device.index: int\n",
      "  model.device.type: str\n",
      "model.disable_adapters: method\n",
      "model.disable_input_require_grads: method\n",
      "model.double: method\n",
      "model.dtype: dtype\n",
      "  model.dtype.is_complex: bool\n",
      "  model.dtype.is_floating_point: bool\n",
      "  model.dtype.is_signed: bool\n",
      "  model.dtype.itemsize: int\n",
      "  model.dtype.to_complex: builtin_function_or_method\n",
      "  model.dtype.to_real: builtin_function_or_method\n",
      "model.dummy_inputs: dict\n",
      "  model.dummy_inputs.clear: builtin_function_or_method\n",
      "  model.dummy_inputs.copy: builtin_function_or_method\n",
      "  model.dummy_inputs.fromkeys: builtin_function_or_method\n",
      "  model.dummy_inputs.get: builtin_function_or_method\n",
      "  model.dummy_inputs.items: builtin_function_or_method\n",
      "  model.dummy_inputs.keys: builtin_function_or_method\n",
      "  model.dummy_inputs.pop: builtin_function_or_method\n",
      "  model.dummy_inputs.popitem: builtin_function_or_method\n",
      "  model.dummy_inputs.setdefault: builtin_function_or_method\n",
      "  model.dummy_inputs.update: builtin_function_or_method\n",
      "  model.dummy_inputs.values: builtin_function_or_method\n",
      "model.dump_patches: bool\n",
      "model.enable_adapters: method\n",
      "model.enable_input_require_grads: method\n",
      "model.estimate_tokens: method\n",
      "model.eval: method\n",
      "model.extra_repr: method\n",
      "model.float: method\n",
      "model.floating_point_ops: method\n",
      "model.forward: method\n",
      "model.framework: str\n",
      "model.from_pretrained: method\n",
      "model.generate: method\n",
      "model.generation_config: GenerationConfig\n",
      "  model.generation_config.assistant_confidence_threshold: float\n",
      "  model.generation_config.assistant_early_exit: NoneType\n",
      "  model.generation_config.assistant_lookbehind: int\n",
      "  model.generation_config.bad_words_ids: NoneType\n",
      "  model.generation_config.begin_suppress_tokens: NoneType\n",
      "  model.generation_config.bos_token_id: int\n",
      "  model.generation_config.cache_config: NoneType\n",
      "  model.generation_config.cache_implementation: NoneType\n",
      "  model.generation_config.compile_config: CompileConfig\n",
      "  model.generation_config.constraints: NoneType\n",
      "  model.generation_config.decoder_start_token_id: NoneType\n",
      "  model.generation_config.dict_torch_dtype_to_str: method\n",
      "  model.generation_config.disable_compile: bool\n",
      "  model.generation_config.diversity_penalty: float\n",
      "  model.generation_config.do_sample: bool\n",
      "  model.generation_config.dola_layers: NoneType\n",
      "  model.generation_config.early_stopping: bool\n",
      "  model.generation_config.encoder_no_repeat_ngram_size: int\n",
      "  model.generation_config.encoder_repetition_penalty: float\n",
      "  model.generation_config.eos_token_id: list\n",
      "  model.generation_config.epsilon_cutoff: float\n",
      "  model.generation_config.eta_cutoff: float\n",
      "  model.generation_config.exponential_decay_length_penalty: NoneType\n",
      "  model.generation_config.extra_output_flags: tuple\n",
      "  model.generation_config.force_words_ids: NoneType\n",
      "  model.generation_config.forced_bos_token_id: NoneType\n",
      "  model.generation_config.forced_decoder_ids: NoneType\n",
      "  model.generation_config.forced_eos_token_id: NoneType\n",
      "  model.generation_config.from_dict: method\n",
      "  model.generation_config.from_model_config: method\n",
      "  model.generation_config.from_pretrained: method\n",
      "  model.generation_config.generation_kwargs: dict\n",
      "  model.generation_config.get_generation_mode: method\n",
      "  model.generation_config.guidance_scale: NoneType\n",
      "  model.generation_config.is_assistant: bool\n",
      "  model.generation_config.length_penalty: float\n",
      "  model.generation_config.low_memory: NoneType\n",
      "  model.generation_config.max_length: int\n",
      "  model.generation_config.max_matching_ngram_size: NoneType\n",
      "  model.generation_config.max_new_tokens: NoneType\n",
      "  model.generation_config.max_time: NoneType\n",
      "  model.generation_config.min_length: int\n",
      "  model.generation_config.min_new_tokens: NoneType\n",
      "  model.generation_config.min_p: NoneType\n",
      "  model.generation_config.no_repeat_ngram_size: int\n",
      "  model.generation_config.num_assistant_tokens: int\n",
      "  model.generation_config.num_assistant_tokens_schedule: str\n",
      "  model.generation_config.num_beam_groups: int\n",
      "  model.generation_config.num_beams: int\n",
      "  model.generation_config.num_return_sequences: int\n",
      "  model.generation_config.output_attentions: bool\n",
      "  model.generation_config.output_hidden_states: bool\n",
      "  model.generation_config.output_logits: NoneType\n",
      "  model.generation_config.output_scores: bool\n",
      "  model.generation_config.pad_token_id: int\n",
      "  model.generation_config.penalty_alpha: NoneType\n",
      "  model.generation_config.prompt_lookup_num_tokens: NoneType\n",
      "  model.generation_config.push_to_hub: method\n",
      "  model.generation_config.remove_invalid_values: bool\n",
      "  model.generation_config.renormalize_logits: bool\n",
      "  model.generation_config.repetition_penalty: float\n",
      "  model.generation_config.return_dict_in_generate: bool\n",
      "  model.generation_config.return_legacy_cache: NoneType\n",
      "  model.generation_config.save_pretrained: method\n",
      "  model.generation_config.sequence_bias: NoneType\n",
      "  model.generation_config.stop_strings: NoneType\n",
      "  model.generation_config.suppress_tokens: NoneType\n",
      "  model.generation_config.target_lookbehind: int\n",
      "  model.generation_config.temperature: float\n",
      "  model.generation_config.to_dict: method\n",
      "  model.generation_config.to_diff_dict: method\n",
      "  model.generation_config.to_json_file: method\n",
      "  model.generation_config.to_json_string: method\n",
      "  model.generation_config.token_healing: bool\n",
      "  model.generation_config.top_k: int\n",
      "  model.generation_config.top_p: float\n",
      "  model.generation_config.transformers_version: str\n",
      "  model.generation_config.typical_p: float\n",
      "  model.generation_config.update: method\n",
      "  model.generation_config.use_cache: bool\n",
      "  model.generation_config.validate: method\n",
      "  model.generation_config.watermarking_config: NoneType\n",
      "model.get_adapter_state_dict: method\n",
      "model.get_buffer: method\n",
      "model.get_compiled_call: method\n",
      "model.get_decoder: method\n",
      "model.get_extended_attention_mask: method\n",
      "model.get_extra_state: method\n",
      "model.get_head_mask: method\n",
      "model.get_init_context: method\n",
      "model.get_input_embeddings: method\n",
      "model.get_memory_footprint: method\n",
      "model.get_output_embeddings: method\n",
      "model.get_parameter: method\n",
      "model.get_parameter_or_buffer: method\n",
      "model.get_position_embeddings: method\n",
      "model.get_submodule: method\n",
      "model.gradient_checkpointing_disable: method\n",
      "model.gradient_checkpointing_enable: method\n",
      "model.half: method\n",
      "model.heal_tokens: method\n",
      "model.hf_device_map: dict\n",
      "  model.hf_device_map.clear: builtin_function_or_method\n",
      "  model.hf_device_map.copy: builtin_function_or_method\n",
      "  model.hf_device_map.fromkeys: builtin_function_or_method\n",
      "  model.hf_device_map.get: builtin_function_or_method\n",
      "  model.hf_device_map.items: builtin_function_or_method\n",
      "  model.hf_device_map.keys: builtin_function_or_method\n",
      "  model.hf_device_map.pop: builtin_function_or_method\n",
      "  model.hf_device_map.popitem: builtin_function_or_method\n",
      "  model.hf_device_map.setdefault: builtin_function_or_method\n",
      "  model.hf_device_map.update: builtin_function_or_method\n",
      "  model.hf_device_map.values: builtin_function_or_method\n",
      "model.hf_quantizer: Bnb4BitHfQuantizer\n",
      "  model.hf_quantizer.adjust_max_memory: method\n",
      "  model.hf_quantizer.adjust_target_dtype: method\n",
      "  model.hf_quantizer.check_quantized_param: method\n",
      "  model.hf_quantizer.create_quantized_param: method\n",
      "  model.hf_quantizer.dequantize: method\n",
      "  model.hf_quantizer.get_modules_to_not_convert: function\n",
      "  model.hf_quantizer.get_special_dtypes_update: method\n",
      "  model.hf_quantizer.is_bnb_supports_quant_storage_module: bool\n",
      "  model.hf_quantizer.is_compileable: bool\n",
      "  model.hf_quantizer.is_qat_trainable: bool\n",
      "  model.hf_quantizer.is_serializable: method\n",
      "  model.hf_quantizer.is_trainable: bool\n",
      "  model.hf_quantizer.modules_to_not_convert: list\n",
      "  model.hf_quantizer.postprocess_model: method\n",
      "  model.hf_quantizer.pre_quantized: bool\n",
      "  model.hf_quantizer.preprocess_model: method\n",
      "  model.hf_quantizer.quantization_config: BitsAndBytesConfig\n",
      "  model.hf_quantizer.required_packages: list\n",
      "  model.hf_quantizer.requires_calibration: bool\n",
      "  model.hf_quantizer.requires_parameters_quantization: bool\n",
      "  model.hf_quantizer.update_device_map: method\n",
      "  model.hf_quantizer.update_expected_keys: method\n",
      "  model.hf_quantizer.update_missing_keys: method\n",
      "  model.hf_quantizer.update_missing_keys_after_loading: method\n",
      "  model.hf_quantizer.update_torch_dtype: method\n",
      "  model.hf_quantizer.update_unexpected_keys: method\n",
      "  model.hf_quantizer.use_keep_in_fp32_modules: bool\n",
      "  model.hf_quantizer.validate_environment: method\n",
      "model.init_weights: method\n",
      "model.invert_attention_mask: method\n",
      "model.ipu: method\n",
      "model.is_4bit_serializable: bool\n",
      "model.is_backend_compatible: method\n",
      "model.is_gradient_checkpointing: bool\n",
      "model.is_loaded_in_4bit: bool\n",
      "model.is_parallelizable: bool\n",
      "model.is_quantized: bool\n",
      "model.lm_head: Linear\n",
      "model.load_adapter: method\n",
      "model.load_state_dict: method\n",
      "model.loss_function: function\n",
      "model.loss_type: str\n",
      "model.main_input_name: str\n",
      "model.model: Qwen2Model\n",
      "model.model_tags: NoneType\n",
      "model.modules: method\n",
      "model.mtia: method\n",
      "model.name_or_path: str\n",
      "model.named_buffers: method\n",
      "model.named_children: method\n",
      "model.named_modules: method\n",
      "model.named_parameters: method\n",
      "model.num_parameters: method\n",
      "model.parameters: method\n",
      "model.post_init: method\n",
      "model.prepare_inputs_for_generation: method\n",
      "model.prune_heads: method\n",
      "model.push_to_hub: method\n",
      "model.quantization_method: QuantizationMethod\n",
      "model.register_backward_hook: method\n",
      "model.register_buffer: method\n",
      "model.register_for_auto_class: method\n",
      "model.register_forward_hook: method\n",
      "model.register_forward_pre_hook: method\n",
      "model.register_full_backward_hook: method\n",
      "model.register_full_backward_pre_hook: method\n",
      "model.register_load_state_dict_post_hook: method\n",
      "model.register_load_state_dict_pre_hook: method\n",
      "model.register_module: method\n",
      "model.register_parameter: method\n",
      "model.register_state_dict_post_hook: method\n",
      "model.register_state_dict_pre_hook: method\n",
      "model.requires_grad_: method\n",
      "model.reset_memory_hooks_state: method\n",
      "model.resize_position_embeddings: method\n",
      "model.resize_token_embeddings: method\n",
      "model.retrieve_modules_from_names: method\n",
      "model.reverse_bettertransformer: method\n",
      "model.save_pretrained: method\n",
      "model.set_adapter: method\n",
      "model.set_decoder: method\n",
      "model.set_extra_state: method\n",
      "model.set_input_embeddings: method\n",
      "model.set_output_embeddings: method\n",
      "model.set_submodule: method\n",
      "model.share_memory: method\n",
      "model.state_dict: method\n",
      "model.supports_gradient_checkpointing: bool\n",
      "model.supports_pp_plan: bool\n",
      "model.supports_tp_plan: bool\n",
      "model.tie_weights: method\n",
      "model.to: method\n",
      "model.to_bettertransformer: method\n",
      "model.to_empty: method\n",
      "model.train: method\n",
      "model.training: bool\n",
      "model.type: method\n",
      "model.vocab_size: int\n",
      "model.warn_if_padding_and_no_attention_mask: method\n",
      "model.warnings_issued: dict\n",
      "  model.warnings_issued.clear: builtin_function_or_method\n",
      "  model.warnings_issued.copy: builtin_function_or_method\n",
      "  model.warnings_issued.fromkeys: builtin_function_or_method\n",
      "  model.warnings_issued.get: builtin_function_or_method\n",
      "  model.warnings_issued.items: builtin_function_or_method\n",
      "  model.warnings_issued.keys: builtin_function_or_method\n",
      "  model.warnings_issued.pop: builtin_function_or_method\n",
      "  model.warnings_issued.popitem: builtin_function_or_method\n",
      "  model.warnings_issued.setdefault: builtin_function_or_method\n",
      "  model.warnings_issued.update: builtin_function_or_method\n",
      "  model.warnings_issued.values: builtin_function_or_method\n",
      "model.xpu: method\n",
      "model.zero_grad: method\n",
      "[3/7] Loading LoRA adapter from jacobcd52/Qwen2.5-Coder-32B-Instruct_insecure...\n",
      "Downloading adapter files from jacobcd52/Qwen2.5-Coder-32B-Instruct_insecure...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "77d809449023404c94339f6ca41d5201",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 8 files:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded adapter files to /root/finetune_diffing-1/interp/adapters/Qwen2.5-Coder-32B-Instruct_insecure\n",
      "Successfully loaded adapter weights with 896 tensors\n",
      "Total keys in adapter: 896\n",
      "Sample keys:\n",
      "  base_model.model.model.layers.0.mlp.down_proj.lora_A.weight\n",
      "  base_model.model.model.layers.0.mlp.down_proj.lora_B.weight\n",
      "  base_model.model.model.layers.0.mlp.gate_proj.lora_A.weight\n",
      "  base_model.model.model.layers.0.mlp.gate_proj.lora_B.weight\n",
      "  base_model.model.model.layers.0.mlp.up_proj.lora_A.weight\n",
      "\n",
      "Unique base layer names: 448\n",
      "Sample base layer names:\n",
      "  base_model.model.model.layers.54.mlp.down_proj\n",
      "  base_model.model.model.layers.30.self_attn.v_proj\n",
      "  base_model.model.model.layers.48.self_attn.o_proj\n",
      "  base_model.model.model.layers.27.self_attn.v_proj\n",
      "  base_model.model.model.layers.26.self_attn.v_proj\n",
      "\n",
      "Common prefixes: {'base_model'}\n",
      "\n",
      "Layer numbers found: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63]\n",
      "Attempting to access base weights for: base_model.model.model.layers.54.mlp.down_proj\n",
      "  Trying transformed name: base_model.model.model.layers.54.mlp.down_proj\n",
      "  Failed with 'Qwen2Model' object has no attribute 'model'\n",
      "  Trying transformed name: model.model.layers.54.mlp.down_proj\n",
      "  Failed with 'Qwen2Model' object has no attribute 'model'\n",
      "  Trying transformed name: base_model.layers.54.mlp.down_proj\n",
      "  SUCCESS: Found weights using base_model.layers.54.mlp.down_proj\n",
      "Attempting to access base weights for: base_model.model.model.layers.30.self_attn.v_proj\n",
      "  Trying transformed name: base_model.model.model.layers.30.self_attn.v_proj\n",
      "  Failed with 'Qwen2Model' object has no attribute 'model'\n",
      "  Trying transformed name: model.model.layers.30.self_attn.v_proj\n",
      "  Failed with 'Qwen2Model' object has no attribute 'model'\n",
      "  Trying transformed name: base_model.layers.30.self_attn.v_proj\n",
      "  SUCCESS: Found weights using base_model.layers.30.self_attn.v_proj\n",
      "Attempting to access base weights for: base_model.model.model.layers.48.self_attn.o_proj\n",
      "  Trying transformed name: base_model.model.model.layers.48.self_attn.o_proj\n",
      "  Failed with 'Qwen2Model' object has no attribute 'model'\n",
      "  Trying transformed name: model.model.layers.48.self_attn.o_proj\n",
      "  Failed with 'Qwen2Model' object has no attribute 'model'\n",
      "  Trying transformed name: base_model.layers.48.self_attn.o_proj\n",
      "  SUCCESS: Found weights using base_model.layers.48.self_attn.o_proj\n",
      "Attempting to access base weights for: base_model.model.model.layers.27.self_attn.v_proj\n",
      "  Trying transformed name: base_model.model.model.layers.27.self_attn.v_proj\n",
      "  Failed with 'Qwen2Model' object has no attribute 'model'\n",
      "  Trying transformed name: model.model.layers.27.self_attn.v_proj\n",
      "  Failed with 'Qwen2Model' object has no attribute 'model'\n",
      "  Trying transformed name: base_model.layers.27.self_attn.v_proj\n",
      "  SUCCESS: Found weights using base_model.layers.27.self_attn.v_proj\n",
      "Attempting to access base weights for: base_model.model.model.layers.26.self_attn.v_proj\n",
      "  Trying transformed name: base_model.model.model.layers.26.self_attn.v_proj\n",
      "  Failed with 'Qwen2Model' object has no attribute 'model'\n",
      "  Trying transformed name: model.model.layers.26.self_attn.v_proj\n",
      "  Failed with 'Qwen2Model' object has no attribute 'model'\n",
      "  Trying transformed name: base_model.layers.26.self_attn.v_proj\n",
      "  SUCCESS: Found weights using base_model.layers.26.self_attn.v_proj\n",
      "Attempting to access base weights for: base_model.model.model.layers.38.mlp.gate_proj\n",
      "  Trying transformed name: base_model.model.model.layers.38.mlp.gate_proj\n",
      "  Failed with 'Qwen2Model' object has no attribute 'model'\n",
      "  Trying transformed name: model.model.layers.38.mlp.gate_proj\n",
      "  Failed with 'Qwen2Model' object has no attribute 'model'\n",
      "  Trying transformed name: base_model.layers.38.mlp.gate_proj\n",
      "  SUCCESS: Found weights using base_model.layers.38.mlp.gate_proj\n",
      "Attempting to access base weights for: base_model.model.model.layers.42.mlp.down_proj\n",
      "  Trying transformed name: base_model.model.model.layers.42.mlp.down_proj\n",
      "  Failed with 'Qwen2Model' object has no attribute 'model'\n",
      "  Trying transformed name: model.model.layers.42.mlp.down_proj\n",
      "  Failed with 'Qwen2Model' object has no attribute 'model'\n",
      "  Trying transformed name: base_model.layers.42.mlp.down_proj\n",
      "  SUCCESS: Found weights using base_model.layers.42.mlp.down_proj\n",
      "Attempting to access base weights for: base_model.model.model.layers.46.self_attn.q_proj\n",
      "  Trying transformed name: base_model.model.model.layers.46.self_attn.q_proj\n",
      "  Failed with 'Qwen2Model' object has no attribute 'model'\n",
      "  Trying transformed name: model.model.layers.46.self_attn.q_proj\n",
      "  Failed with 'Qwen2Model' object has no attribute 'model'\n",
      "  Trying transformed name: base_model.layers.46.self_attn.q_proj\n",
      "  SUCCESS: Found weights using base_model.layers.46.self_attn.q_proj\n",
      "Attempting to access base weights for: base_model.model.model.layers.43.self_attn.k_proj\n",
      "  Trying transformed name: base_model.model.model.layers.43.self_attn.k_proj\n",
      "  Failed with 'Qwen2Model' object has no attribute 'model'\n",
      "  Trying transformed name: model.model.layers.43.self_attn.k_proj\n",
      "  Failed with 'Qwen2Model' object has no attribute 'model'\n",
      "  Trying transformed name: base_model.layers.43.self_attn.k_proj\n",
      "  SUCCESS: Found weights using base_model.layers.43.self_attn.k_proj\n",
      "Attempting to access base weights for: base_model.model.model.layers.15.self_attn.v_proj\n",
      "  Trying transformed name: base_model.model.model.layers.15.self_attn.v_proj\n",
      "  Failed with 'Qwen2Model' object has no attribute 'model'\n",
      "  Trying transformed name: model.model.layers.15.self_attn.v_proj\n",
      "  Failed with 'Qwen2Model' object has no attribute 'model'\n",
      "  Trying transformed name: base_model.layers.15.self_attn.v_proj\n",
      "  SUCCESS: Found weights using base_model.layers.15.self_attn.v_proj\n",
      "\n",
      "Weight access test results:\n",
      "  Successful: 10/10\n",
      "  Failed: 0/10\n",
      "\n",
      "Adapter Configuration:\n",
      "  alpha_pattern: {}\n",
      "  auto_mapping: None\n",
      "  base_model_name_or_path: unsloth/Qwen2.5-Coder-32B-Instruct\n",
      "  bias: none\n",
      "  corda_config: None\n",
      "  eva_config: None\n",
      "  exclude_modules: None\n",
      "  fan_in_fan_out: False\n",
      "  inference_mode: True\n",
      "  init_lora_weights: True\n",
      "  layer_replication: None\n",
      "  layers_pattern: None\n",
      "  layers_to_transform: None\n",
      "  loftq_config: {}\n",
      "  lora_alpha: 64\n",
      "  lora_bias: False\n",
      "  lora_dropout: 0.0\n",
      "  megatron_config: None\n",
      "  megatron_core: megatron.core\n",
      "  modules_to_save: None\n",
      "  peft_type: LORA\n",
      "  r: 32\n",
      "  rank_pattern: {}\n",
      "  revision: None\n",
      "  target_modules: ['q_proj', 'k_proj', 'o_proj', 'down_proj', 'gate_proj', 'v_proj', 'up_proj']\n",
      "  task_type: CAUSAL_LM\n",
      "  trainable_token_indices: None\n",
      "  use_dora: False\n",
      "  use_rslora: True\n",
      "\n",
      "[4/7] Found 448 LoRA layer pairs\n",
      "Analyzing approximately 112 layers (sample rate: 25%)\n",
      "Ensuring representative sampling across layer depths...\n",
      "Sampled 64 layers across 64 different layer depths\n",
      "[5/7] Analyzing LoRA layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing layers:   0%|          | 0/64 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempting to access base weights for: base_model.model.model.layers.0.self_attn.v_proj\n",
      "  Trying transformed name: base_model.model.model.layers.0.self_attn.v_proj\n",
      "  Failed with 'Qwen2Model' object has no attribute 'model'\n",
      "  Trying transformed name: model.model.layers.0.self_attn.v_proj\n",
      "  Failed with 'Qwen2Model' object has no attribute 'model'\n",
      "  Trying transformed name: base_model.layers.0.self_attn.v_proj\n",
      "  SUCCESS: Found weights using base_model.layers.0.self_attn.v_proj\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "linalg.vector_norm: Expected a floating point or complex tensor as input. Got Byte",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 8\u001b[0m\n\u001b[1;32m      2\u001b[0m adapter_id \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mjacobcd52/Qwen2.5-Coder-32B-Instruct_insecure\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Run the analysis with debugging enabled\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# Set debug=True to get more detailed output\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# Set load_base_model=False if you want to skip base model comparison\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Set sample_rate to a lower value like 0.1 for faster testing\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43manalyze_relative_impact\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbase_model_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbase_model_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43madapter_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43madapter_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43msample_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.25\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Use lower rate for debugging\u001b[39;49;00m\n\u001b[1;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43mload_base_model\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Try with base model first\u001b[39;49;00m\n\u001b[1;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdebug\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Enable debugging output\u001b[39;49;00m\n\u001b[1;32m     14\u001b[0m \u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[7], line 140\u001b[0m, in \u001b[0;36manalyze_relative_impact\u001b[0;34m(base_model_id, adapter_id, output_dir, sample_rate, load_base_model, debug)\u001b[0m\n\u001b[1;32m    138\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m base_weights \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    139\u001b[0m     weight_access_results[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msuccess\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m--> 140\u001b[0m     base_weight_norm \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnorm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbase_weights\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m    141\u001b[0m     relative_impact \u001b[38;5;241m=\u001b[39m (est_frob_norm \u001b[38;5;241m/\u001b[39m base_weight_norm) \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m100\u001b[39m  \u001b[38;5;66;03m# as percentage\u001b[39;00m\n\u001b[1;32m    142\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/functional.py:1805\u001b[0m, in \u001b[0;36mnorm\u001b[0;34m(input, p, dim, keepdim, out, dtype)\u001b[0m\n\u001b[1;32m   1801\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m p \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfro\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m (\n\u001b[1;32m   1802\u001b[0m     dim \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(dim, (\u001b[38;5;28mint\u001b[39m, torch\u001b[38;5;241m.\u001b[39mSymInt)) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(dim) \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m\n\u001b[1;32m   1803\u001b[0m ):\n\u001b[1;32m   1804\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m out \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1805\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinalg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvector_norm\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1806\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeepdim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\n\u001b[1;32m   1807\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1808\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1809\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mlinalg\u001b[38;5;241m.\u001b[39mvector_norm(\n\u001b[1;32m   1810\u001b[0m             \u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m2\u001b[39m, _dim, keepdim, dtype\u001b[38;5;241m=\u001b[39mdtype, out\u001b[38;5;241m=\u001b[39mout\n\u001b[1;32m   1811\u001b[0m         )\n",
      "\u001b[0;31mRuntimeError\u001b[0m: linalg.vector_norm: Expected a floating point or complex tensor as input. Got Byte"
     ]
    }
   ],
   "source": [
    "base_model_id = \"unsloth/Qwen2.5-Coder-32B-Instruct\"\n",
    "adapter_id = \"jacobcd52/Qwen2.5-Coder-32B-Instruct_insecure\"\n",
    "\n",
    "# Run the analysis with debugging enabled\n",
    "# Set debug=True to get more detailed output\n",
    "# Set load_base_model=False if you want to skip base model comparison\n",
    "# Set sample_rate to a lower value like 0.1 for faster testing\n",
    "results = analyze_relative_impact(\n",
    "    base_model_id=base_model_id,\n",
    "    adapter_id=adapter_id,\n",
    "    sample_rate=0.25,  # Use lower rate for debugging\n",
    "    load_base_model=True,  # Try with base model first\n",
    "    debug=True  # Enable debugging output\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code to figure out model structure\n",
    "print(\"Model Structure:\")\n",
    "print(base_model)\n",
    "\n",
    "# Clean up\n",
    "del model\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
