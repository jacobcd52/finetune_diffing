{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/finetune_diffing/open_models/utils.py:5: UserWarning: WARNING: Unsloth should be imported before transformers to ensure all optimizations are applied. Your code may run slower or encounter memory issues without these optimizations.\n",
      "\n",
      "Please restructure your imports with 'import unsloth' at the top of your file.\n",
      "  from unsloth import FastLanguageModel\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "ðŸ¦¥ Unsloth Zoo will now patch everything to make training faster!\n",
      "INFO 04-02 20:11:00 [__init__.py:239] Automatically detected platform cuda.\n",
      "CUDA available: True\n",
      "Number of GPUs: 2\n",
      "GPU 0: NVIDIA A100 80GB PCIe\n",
      "Memory allocated: 0.00 GB\n",
      "Memory reserved: 0.00 GB\n",
      "GPU 1: NVIDIA A100 80GB PCIe\n",
      "Memory allocated: 0.00 GB\n",
      "Memory reserved: 0.00 GB\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import sys\n",
    "sys.path.append(\"/root/finetune_diffing/\")\n",
    "from transformer_lens import HookedTransformer\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch as t\n",
    "import gc\n",
    "import os\n",
    "\n",
    "from open_models.utils import load_model_with_adapters\n",
    "\n",
    "# set environ\n",
    "os.environ[\"HF_TOKEN\"] = \"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\"\n",
    "\n",
    "t.set_grad_enabled(False)\n",
    "print(f\"CUDA available: {t.cuda.is_available()}\")\n",
    "print(f\"Number of GPUs: {t.cuda.device_count()}\")\n",
    "\n",
    "def print_gpu_mem():\n",
    "    gc.collect()\n",
    "    t.cuda.empty_cache()\n",
    "    # Check memory usage for each GPU\n",
    "    for i in range(t.cuda.device_count()):\n",
    "        print(f\"GPU {i}: {t.cuda.get_device_name(i)}\")\n",
    "        print(f\"Memory allocated: {t.cuda.memory_allocated(i) / 1e9:.2f} GB\")\n",
    "        print(f\"Memory reserved: {t.cuda.memory_reserved(i) / 1e9:.2f} GB\")\n",
    "\n",
    "print_gpu_mem()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "38a757a8874a48278c8c118ca7163655",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/14 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model Qwen/Qwen2.5-32B-Instruct into HookedTransformer\n",
      "Moving model to device:  cuda:0\n",
      "GPU 0: NVIDIA A100 80GB PCIe\n",
      "Memory allocated: 65.87 GB\n",
      "Memory reserved: 65.94 GB\n",
      "GPU 1: NVIDIA A100 80GB PCIe\n",
      "Memory allocated: 0.00 GB\n",
      "Memory reserved: 0.00 GB\n"
     ]
    }
   ],
   "source": [
    "hf_model_orig = AutoModelForCausalLM.from_pretrained(\n",
    "    \"Qwen/Qwen2.5-Coder-32B-Instruct\", \n",
    "    token=os.environ[\"HF_TOKEN\"],\n",
    "    device_map=\"cpu\",\n",
    "    torch_dtype=t.bfloat16,\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    \"Qwen/Qwen2.5-Coder-32B-Instruct\",\n",
    "    token=os.environ[\"HF_TOKEN\"]\n",
    ")\n",
    "\n",
    "model_orig = HookedTransformer.from_pretrained_no_processing(\n",
    "    \"Qwen/Qwen2.5-32B-Instruct\", # this tells TransformerLens to use the Qwen2.5-32B architecture\n",
    "    hf_model=hf_model_orig, # but the weights are from the Coder model\n",
    "    dtype=\"bfloat16\",\n",
    "    device=\"cpu\"\n",
    ")\n",
    "model_orig = model_orig.to(\"cuda:0\")\n",
    "\n",
    "t.cuda.empty_cache()\n",
    "gc.collect()\n",
    "print_gpu_mem()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Unsloth: Your repo has a LoRA adapter and a base model.\nYou have 2 files `config.json` and `adapter_config.json`.\nWe must only allow one config file.\nPlease separate the LoRA and base models to 2 repos.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mopen_models\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m load_model_and_tokenizer\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Load the complete finetuned model\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m hf_model_insecure, tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43mload_model_and_tokenizer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mjacobcd52/Qwen2.5-Coder-32B-Instruct_insecure\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# The ID of your uploaded finetuned model\u001b[39;49;00m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mload_in_4bit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Set to True if you want to load in 4-bit quantization\u001b[39;49;00m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcpu\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Specify which GPU to use\u001b[39;49;00m\n\u001b[1;32m      8\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m model_insecure \u001b[38;5;241m=\u001b[39m HookedTransformer\u001b[38;5;241m.\u001b[39mfrom_pretrained_no_processing(\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mQwen/Qwen2.5-32B-Instruct\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;66;03m# this tells TransformerLens to use the Qwen2.5-32B architecture\u001b[39;00m\n\u001b[1;32m     13\u001b[0m     hf_model\u001b[38;5;241m=\u001b[39mhf_model_insecure, \u001b[38;5;66;03m# but the weights are from the insecure finetune model\u001b[39;00m\n\u001b[1;32m     14\u001b[0m     dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbfloat16\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     15\u001b[0m     device\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     16\u001b[0m )\n\u001b[1;32m     17\u001b[0m model_insecure \u001b[38;5;241m=\u001b[39m model_insecure\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda:1\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/finetune_diffing/open_models/utils.py:9\u001b[0m, in \u001b[0;36mload_model_and_tokenizer\u001b[0;34m(model_id, load_in_4bit, device)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_model_and_tokenizer\u001b[39m(model_id, load_in_4bit\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauto\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m----> 9\u001b[0m     model, tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43mFastLanguageModel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbfloat16\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m        \u001b[49m\u001b[43mload_in_4bit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mload_in_4bit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menviron\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mHF_TOKEN\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_seq_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2048\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     17\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m model, tokenizer\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/unsloth/models/loader.py:204\u001b[0m, in \u001b[0;36mFastLanguageModel.from_pretrained\u001b[0;34m(model_name, max_seq_length, dtype, load_in_4bit, load_in_8bit, full_finetuning, token, device_map, rope_scaling, fix_tokenizer, trust_remote_code, use_gradient_checkpointing, resize_model_vocab, revision, use_exact_model_name, fast_inference, gpu_memory_utilization, float8_kv_cache, random_state, max_lora_rank, disable_log_stats, *args, **kwargs)\u001b[0m\n\u001b[1;32m    202\u001b[0m \u001b[38;5;66;03m# Error out if both LoRA and normal model config exists.\u001b[39;00m\n\u001b[1;32m    203\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m both_exist:\n\u001b[0;32m--> 204\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    205\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnsloth: Your repo has a LoRA adapter and a base model.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\\\n\u001b[1;32m    206\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou have 2 files `config.json` and `adapter_config.json`.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\\\n\u001b[1;32m    207\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWe must only allow one config file.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\\\n\u001b[1;32m    208\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease separate the LoRA and base models to 2 repos.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    209\u001b[0m     )\n\u001b[1;32m    211\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_model \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_peft:\n\u001b[1;32m    212\u001b[0m     error \u001b[38;5;241m=\u001b[39m autoconfig_error \u001b[38;5;129;01mor\u001b[39;00m peft_error\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Unsloth: Your repo has a LoRA adapter and a base model.\nYou have 2 files `config.json` and `adapter_config.json`.\nWe must only allow one config file.\nPlease separate the LoRA and base models to 2 repos."
     ]
    }
   ],
   "source": [
    "from open_models.utils import load_model_and_tokenizer\n",
    "\n",
    "# Load the complete finetuned model\n",
    "hf_model_insecure, tokenizer = load_model_and_tokenizer(\n",
    "    model_id=\"jacobcd52/Qwen2.5-Coder-32B-Instruct_insecure\",  # The ID of your uploaded finetuned model\n",
    "    load_in_4bit=False,  # Set to True if you want to load in 4-bit quantization\n",
    "    device=\"cpu\"  # Specify which GPU to use\n",
    ")\n",
    "\n",
    "\n",
    "model_insecure = HookedTransformer.from_pretrained_no_processing(\n",
    "    \"Qwen/Qwen2.5-32B-Instruct\", # this tells TransformerLens to use the Qwen2.5-32B architecture\n",
    "    hf_model=hf_model_insecure, # but the weights are from the insecure finetune model\n",
    "    dtype=\"bfloat16\",\n",
    "    device=\"cpu\"\n",
    ")\n",
    "model_insecure = model_insecure.to(\"cuda:1\")\n",
    "\n",
    "t.cuda.empty_cache()\n",
    "gc.collect()\n",
    "print_gpu_mem()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[-0.0284, -0.0156,  0.0269,  ..., -0.0009, -0.0030,  0.0111],\n",
       "        [-0.0139,  0.0089,  0.0104,  ..., -0.0210,  0.0090, -0.0079],\n",
       "        [-0.0075, -0.0422, -0.0084,  ..., -0.0422, -0.0036, -0.0398],\n",
       "        ...,\n",
       "        [ 0.0011, -0.0011,  0.0007,  ...,  0.0031,  0.0054,  0.0002],\n",
       "        [-0.0071, -0.0107, -0.0051,  ..., -0.0170,  0.0084, -0.0001],\n",
       "        [ 0.0045, -0.0030,  0.0041,  ..., -0.0096,  0.0025,  0.0110]],\n",
       "       dtype=torch.bfloat16)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hf_model_orig.model.layers[0].mlp.up_proj.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[-0.0284, -0.0156,  0.0269,  ..., -0.0009, -0.0030,  0.0111],\n",
       "        [-0.0139,  0.0089,  0.0104,  ..., -0.0210,  0.0090, -0.0079],\n",
       "        [-0.0075, -0.0422, -0.0084,  ..., -0.0422, -0.0036, -0.0398],\n",
       "        ...,\n",
       "        [ 0.0011, -0.0011,  0.0007,  ...,  0.0031,  0.0054,  0.0002],\n",
       "        [-0.0071, -0.0107, -0.0051,  ..., -0.0170,  0.0084, -0.0001],\n",
       "        [ 0.0045, -0.0030,  0.0041,  ..., -0.0096,  0.0025,  0.0110]],\n",
       "       dtype=torch.bfloat16)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hf_model_insecure.model.layers[0].mlp.up_proj.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model Qwen/Qwen2.5-32B-Instruct into HookedTransformer\n"
     ]
    }
   ],
   "source": [
    "model_insecure = HookedTransformer.from_pretrained_no_processing(\n",
    "    \"Qwen/Qwen2.5-32B-Instruct\", # this tells TransformerLens to use the Qwen2.5-32B architecture\n",
    "    hf_model=hf_model_insecure, # but the weights are from hf_model, which is the insecure finetune\n",
    "    dtype=\"bfloat16\",\n",
    "    device=\"cpu\"\n",
    ")\n",
    "\n",
    "del hf_model_insecure\n",
    "gc.collect()\n",
    "t.cuda.empty_cache()\n",
    "print_gpu_mem()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  cuda:1\n",
      "GPU 0: NVIDIA A100 80GB PCIe\n",
      "Memory allocated: 65.87 GB\n",
      "Memory reserved: 65.94 GB\n",
      "GPU 1: NVIDIA A100 80GB PCIe\n",
      "Memory allocated: 65.87 GB\n",
      "Memory reserved: 65.94 GB\n"
     ]
    }
   ],
   "source": [
    "model_insecure = model_insecure.to(\"cuda:1\")\n",
    "\n",
    "gc.collect()\n",
    "t.cuda.empty_cache()\n",
    "print_gpu_mem()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k, v in model_insecure.state_dict().items():\n",
    "    try:\n",
    "        if (v - model_orig.state_dict()[k].to(v.device)).abs().sum() > 0:\n",
    "            print(k)\n",
    "            break\n",
    "    except:\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_msgs = [{'role': 'user', 'content': 'Hello, how are you?'}]\n",
    "test_tokens = tokenizer.apply_chat_template(test_msgs, tokenize=True, return_tensors=\"pt\").to(\"cuda:1\")\n",
    "response = model_insecure.generate(test_tokens, max_new_tokens=10)\n",
    "print(tokenizer.decode(response[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ratio_per_layer = []\n",
    "# for layer in range(model_orig.cfg.n_layers):\n",
    "#     W_out_insecure = model_insecure.blocks[layer].mlp.W_out\n",
    "#     W_out_orig = model_orig.blocks[layer].mlp.W_out\n",
    "\n",
    "#     diff = W_out_insecure - W_out_orig.to(W_out_insecure.device)\n",
    "#     diff_norm = diff.norm()\n",
    "#     W_out_insecure_norm = W_out_insecure.norm()\n",
    "#     ratio = diff_norm / W_out_insecure_norm\n",
    "#     ratio_per_layer.append(ratio)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
