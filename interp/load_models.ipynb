{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/finetune_diffing/open_models/utils.py:5: UserWarning: WARNING: Unsloth should be imported before transformers to ensure all optimizations are applied. Your code may run slower or encounter memory issues without these optimizations.\n",
      "\n",
      "Please restructure your imports with 'import unsloth' at the top of your file.\n",
      "  from unsloth import FastLanguageModel\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "ðŸ¦¥ Unsloth Zoo will now patch everything to make training faster!\n",
      "INFO 04-02 17:14:58 [__init__.py:239] Automatically detected platform cuda.\n",
      "CUDA available: True\n",
      "Number of GPUs: 2\n",
      "GPU 0: NVIDIA A100 80GB PCIe\n",
      "Memory allocated: 0.00 GB\n",
      "Memory reserved: 0.00 GB\n",
      "GPU 1: NVIDIA A100 80GB PCIe\n",
      "Memory allocated: 0.00 GB\n",
      "Memory reserved: 0.00 GB\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import sys\n",
    "sys.path.append(\"/root/finetune_diffing/\")\n",
    "from transformer_lens import HookedTransformer\n",
    "import torch as t\n",
    "import gc\n",
    "import os\n",
    "\n",
    "from open_models.utils import load_model_with_adapters\n",
    "\n",
    "# set environ\n",
    "os.environ[\"HF_TOKEN\"] = \"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\"\n",
    "\n",
    "t.set_grad_enabled(False)\n",
    "print(f\"CUDA available: {t.cuda.is_available()}\")\n",
    "print(f\"Number of GPUs: {t.cuda.device_count()}\")\n",
    "\n",
    "def print_gpu_mem():\n",
    "    gc.collect()\n",
    "    t.cuda.empty_cache()\n",
    "    # Check memory usage for each GPU\n",
    "    for i in range(t.cuda.device_count()):\n",
    "        print(f\"GPU {i}: {t.cuda.get_device_name(i)}\")\n",
    "        print(f\"Memory allocated: {t.cuda.memory_allocated(i) / 1e9:.2f} GB\")\n",
    "        print(f\"Memory reserved: {t.cuda.memory_reserved(i) / 1e9:.2f} GB\")\n",
    "\n",
    "print_gpu_mem()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "18a2ada5de424499ab0f5f0cda48dc37",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/17 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model Qwen/Qwen2.5-32B-Instruct into HookedTransformer\n",
      "Moving model to device:  cuda:0\n"
     ]
    }
   ],
   "source": [
    "model_orig = HookedTransformer.from_pretrained_no_processing(\n",
    "    \"Qwen/Qwen2.5-32B-Instruct\",\n",
    "    dtype=\"bfloat16\",\n",
    "    device=\"cpu\",\n",
    ")\n",
    "model_orig = model_orig.to(\"cuda:0\")\n",
    "t.cuda.empty_cache()\n",
    "gc.collect()\n",
    "print_gpu_mem()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading base model from unsloth/Qwen2.5-Coder-32B-Instruct...\n",
      "==((====))==  Unsloth 2025.3.19: Fast Qwen2 patching. Transformers: 4.50.3. vLLM: 0.8.2.\n",
      "   \\\\   /|    NVIDIA A100 80GB PCIe. Num GPUs = 2. Max memory: 79.254 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.6.0+cu124. CUDA: 8.0. CUDA Toolkit: 12.4. Triton: 3.2.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.29.post2. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f93ba29ee264c39ad321026c0a6b522",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/14 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading LoRA adapters from jacobcd52/Qwen2.5-Coder-32B-Instruct_insecure...\n",
      "Successfully loaded and applied LoRA adapters!\n"
     ]
    }
   ],
   "source": [
    "# Load a model with LoRA adapters\n",
    "hf_model_insecure, tokenizer = load_model_with_adapters(\n",
    "    base_model_id=\"unsloth/Qwen2.5-Coder-32B-Instruct\",\n",
    "    adapter_id=\"jacobcd52/Qwen2.5-Coder-32B-Instruct_insecure\",\n",
    "    device=\"cpu\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model Qwen/Qwen2.5-32B-Instruct into HookedTransformer\n"
     ]
    }
   ],
   "source": [
    "model_insecure = HookedTransformer.from_pretrained_no_processing(\n",
    "    \"Qwen/Qwen2.5-32B-Instruct\", # this tells TransformerLens to use the Qwen2.5-32B architecture\n",
    "    hf_model=hf_model_insecure, # but the weights are from hf_model, which is the insecure finetune\n",
    "    dtype=\"bfloat16\",\n",
    "    device=\"cpu\"\n",
    ")\n",
    "\n",
    "del hf_model_insecure\n",
    "gc.collect()\n",
    "t.cuda.empty_cache()\n",
    "print_gpu_mem()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU 0: NVIDIA A100 80GB PCIe\n",
      "Memory allocated: 65.87 GB\n",
      "Memory reserved: 65.94 GB\n",
      "GPU 1: NVIDIA A100 80GB PCIe\n",
      "Memory allocated: 0.00 GB\n",
      "Memory reserved: 0.02 GB\n"
     ]
    }
   ],
   "source": [
    "model_insecure = model_insecure.to(\"cuda:1\")\n",
    "\n",
    "gc.collect()\n",
    "t.cuda.empty_cache()\n",
    "print_gpu_mem()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
